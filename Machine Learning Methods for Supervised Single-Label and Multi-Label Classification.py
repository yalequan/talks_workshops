# -*- coding: utf-8 -*-
"""CSSS_Clustering_2026_02_11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17GbjZBdnPsdOfeYFFsGwUpAcsdPXwl7l
"""

#---- Setup ---
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import pandas as pd
from sklearn.datasets import load_iris

# 1. Load Data
iris = load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
print(df.iloc[1:10])

# --- SLIDE 9: BINARY LOGISTIC REGRESSION ---
X = iris.data[:, :2]   # first two features
y = iris.target

# 2. Create Binary Target: 1 if Virginica (class 2), else 0
y_binary = (y == 2).astype(int)

# 3. Train/Test Split (30% data used for training)
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)

# 4. Train Model
model = LogisticRegression()
model.fit(X_train, y_train)

# 5. Evaluate
y_pred = model.predict(X_test)
print("--- Binary Logistic Regression Results ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print(f"Confusion Matrix:\n{confusion_matrix(y_test, y_pred)}")

# 6. Visualize Decision Boundary (Matches your "Visualizing Logistic Regression" slide)
x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))
Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y_binary, edgecolors='k', cmap=plt.cm.coolwarm)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.title('Binary Logistic Regression Decision Boundary')
plt.show()

# --- SLIDE 10: MULTINOMIAL LOGISTIC REGRESSION ---

# 1. Use the full Iris dataset (3 classes: 0, 1, 2)
# We use the same X as before, but 'y' is now the original 3 classes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 2. Train Model (Softmax is handled automatically by sklearn when multi_class='multinomial')
      # Limited-memory Broyden–Fletcher–Goldfarb–Shanno solver
multi_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
multi_model.fit(X_train, y_train)

# 3. Visualize
Z = multi_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, cmap=plt.cm.viridis, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.viridis)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.title('Multinomial Logistic Regression (3 Classes)')
plt.show()

# --- SLIDE 11: RANDOM FOREST ---
from sklearn.ensemble import RandomForestClassifier

# 1. Train Model (Ensemble of Trees)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# 2. Visualize (Notice the non-linear, "blocky" boundaries)
Z = rf_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, cmap=plt.cm.viridis, alpha=0.8)
plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.viridis)
plt.title('Random Forest Decision Boundary (Non-Linear)')
plt.show()

# --- Slide 12 & 13 Neural Network Perperation: Imports & Scaling ---

# Note: We are using the Keras API, which is my preferred method
# I find it easier to understand than others
# The HUGE downside, is that it requires tensorflow. Tensorflow is
# a fantastic tool for deep learning but it is computationally expensive
# and breaks easily

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Neural Networks REQUIRE scaling!
scaler = StandardScaler() # Z-score scaling (Many others exist)
X = iris.data[:, :2]  # Use only first two features (Sepal Length, Sepal Width) for easy plotting
X_scaled = scaler.fit_transform(X)
print("--- Raw Data ---")
print(X[1:10])
print("--- Scaled Data ---")
print(X_scaled[1:10])

# --- SCENARIO A: BINARY CLASSIFICATION (Virginica vs. Others) ---

# 1. Prepare Labels (0 or 1)
y_binary = (y == 2).astype(int)

# 2. Build Model
model_binary = Sequential([
    # Tells the network the number of features in the dataset
    Input(shape=(2,)),
    # Hidden Layer (Learn patterns)
    # We a priori use 10 neurons in this layer. You can tune for this
    # We also only use 1 layer. You can tune for this
    Dense(10, activation='relu'),
    # Output Layer (Binary Decision)
    # Activation='sigmoid' maps the output to bebetween 0 and 1
    Dense(1, activation='sigmoid')
])

# 3. Compile (Loss = Binary Crossentropy)
model_binary.compile(optimizer=Adam(learning_rate=0.1),
                     loss='binary_crossentropy',
                     metrics=['accuracy'])

# 4. Train
print("--- Training Binary NN ---")
model_binary.fit(X_scaled, y_binary, epochs=50, verbose=0)
print("Training Complete.")

# 5. Visualize Decision Boundary
def plot_keras_boundary(model, X, y, title):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))

    # Predict probabilities
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()], verbose=0)

    # Apply Threshold (0.5) if binary, or argmax if multi-class
    if Z.shape[1] == 1:
        Z = (Z > 0.5).astype(int)
    else:
        Z = np.argmax(Z, axis=1)

    Z = Z.reshape(xx.shape)

    plt.figure(figsize=(8, 6))
    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)
    plt.title(title)
    plt.show()

plot_keras_boundary(model_binary, X_scaled, y_binary, "Binary NN (Sigmoid Output)")

# --- SCENARIO B: MULTI-CLASS (Setosa vs Versicolor vs Virginica) ---

# 1. Build Model
model_multiclass = Sequential([
    Input(shape=(2,)),
    Dense(10, activation='relu'),
    # Output Layer: 3 Nodes (Softmax = Probabilities sum to 1)
    Dense(3, activation='softmax')
])
# 2. Compile
model_multiclass.compile(optimizer=Adam(learning_rate=0.1),
                         loss='sparse_categorical_crossentropy',
                         metrics=['accuracy'])
# 3. Train
print("\n--- Training Multi-Class NN ---")
model_multiclass.fit(X_scaled, y, epochs=50, verbose=0)
print("Training Complete.")
# 4. Visualize
plot_keras_boundary(model_multiclass, X_scaled, y, "Multi-Class NN (Softmax Output)")

# --- SCENARIO C: MULTI-LABEL (Independent Sigmoids) ---

from sklearn.datasets import make_multilabel_classification

# 1. Create Dummy Multi-Label Data
X_multi, y_multi = make_multilabel_classification(n_samples=1000, n_features=2,
                                                  n_classes=3, n_labels=2, random_state=42)
X_multi_scaled = scaler.fit_transform(X_multi)

# 2. Build Model
model_multilabel = Sequential([
    Input(shape=(2,)),
    Dense(10, activation='relu'),
    # Output Layer: 3 independent Nodes with sigmoid
    # Allows multiple labels to be true simultaneously (e.g., [1, 0, 1])
    Dense(3, activation='sigmoid')
])
# 3. Compile (Binary Crossentropy treats each output as an independent)
model_multilabel.compile(optimizer='adam',
                         loss='binary_crossentropy',
                         metrics=['accuracy'])
# 4. Train
print("\n--- Training Multi-Label NN ---")
model_multilabel.fit(X_multi_scaled, y_multi, epochs=20, verbose=0)
# 5. Inspection
print("\n--- Multi-Label Predictions (First 3 samples) ---")
predictions = model_multilabel.predict(X_multi_scaled[:3])
for i, pred in enumerate(predictions):
    print(f"Sample {i+1}:")
    print(f"   Probabilities: {pred.round(2)}")
    # Threshold at 0.5 to decide 'True' or 'False' for each label
    print(f"   Predicted Labels: {(pred > 0.5).astype(int)}")
    print(f"   Actual Labels:    {y_multi[i]}")
    print("-" * 30)

# --- SLIDE 15: K-MEANS CLUSTERING ---
from sklearn.cluster import KMeans

# 1. Train Model
# Note: We do NOT provide 'y' (labels) to fit()
# We assume 3 clusters. You can tune for this
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X) # X contains only features

# 2. Get Cluster Labels
predicted_clusters = kmeans.labels_

# 3. Visualize Clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=predicted_clusters, cmap='viridis', edgecolors='k')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X', label='Centroids')
plt.title('K-Means Clustering (No Labels Used)')
plt.legend()
plt.show()

# --- SLIDE 15: HIERARCHICAL CLUSTERING & DENDROGRAMS ---
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

# 1. Compute Linkage Matrix
# 'ward' minimizes the variance of the clusters being merged
linked = linkage(X, method='ward')

# 2. Plot Dendrogram
plt.figure(figsize=(10, 7))
dendrogram(linked,
           orientation='top',
           distance_sort='descending',
           show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample Index')
plt.ylabel('Distance (Ward)')
plt.show()

# 3. Agglomerative Clustering (Getting Labels)
# If we look at the dendrogram and decide on 3 clusters:
cluster = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
cluster_labels = cluster.fit_predict(X)

# 4. Visualize the Clusters
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='rainbow', edgecolors='k')
plt.title('Hierarchical Clustering (Agglomerative)')
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.show()